---
title: "STAT 108: Final Project Analysis"
author: "Tim Lanthier"
date: "2/8/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Youtube Analysis
```{r message=FALSE}
library(tidyverse)
library(knitr)
library(patchwork)
library(broom)
library(leaps)
```


```{r}
youtube <- read.csv('data/youtube_data.csv') %>%
  subset(select = -c(X)) # removed redundant X column
glimpse(youtube)
```
We will need to change the types of a few of the variables.
```{r}
youtube <- youtube %>%
  mutate(trending_date = as.Date(trending_date),  # strip time since all trending dates recorded at time 0
         publishedAt = as.POSIXct(publishedAt),
         weekday_published = as.factor(weekday_published))
```

We will now drop some of the columns which we will not use in our analysis. 

```{r}
youtube <- subset(youtube, select = -c(video_id, channelId, categoryId, tags, description, title, channelTitle))
```


## Exploratory Data Analysis

Let's start by checking for missing values. When generating some of the features, we automatically dropped videos which no longer exist anymore. 
```{r}
colSums(is.na(youtube))
```
The only column with missing values is `subscriberCount` and `desc_length`. When using the Youtube API to access subscriber counts, for channels with hidden subscriber counts, we decided to input those values of `subscriberCount` as missing. Seeing as we have so few observations, we will drop these from the dataset. For those with missing description length, this is because those videos have no description. So we will replace those with `desc_length` of 0.

```{r}
youtube$desc_length[is.na(youtube$desc_length)] <- 0
youtube <- youtube %>%
  drop_na()
```


### Univariate

We will start by looking at the distributions for the response variable and each of the predictor variables.

```{r}
ggplot(data = youtube, aes(view_count)) + 
  geom_histogram(bins = 100) +
  labs(x = 'View Count', title = 'Distribution of View Count')

summarise(youtube, mean = mean(view_count), 
          std_dev = sd(view_count),
          min = min(view_count),
          q1 = quantile(view_count, 0.25),
          median = median(view_count),
          q3 = quantile(view_count, 0.75),
          max = max(view_count),
          IQR = q3-q1)
```

Looking at the histogram for view count, we see that the distribution is heavily positively skewed. We also have quite a few outliers. Since we wish to conduct inference, we will be looking at the looking at the log view count.

```{r}
youtube <- youtube %>%
  mutate(log_views = log(view_count))

ggplot(data = youtube, aes(log_views))+
  geom_histogram(bins = 30) +
  labs(x = 'Log View Count', title = 'Distribution of Log View Count')
```
We also will investigate the like to dislike ratio. Note that to avoid division by 0, if we have a video with no dislikes, we will let the like to dislike ratio just be the number of likes.

```{r}
youtube <- youtube %>%
  mutate(like_dislike_ratio = ifelse(dislikes == 0, likes, likes/dislikes)) # avoid division by 0
```


```{r}
p1 <- ggplot(data = youtube, aes(likes))+
  geom_histogram(bins = 30) +
  labs(x = 'Number of Likes', title = 'Distribution of Likes')

p2 <- ggplot(data = youtube, aes(dislikes))+
  geom_histogram(bins = 30) +
  labs(x = 'Number of Dislikes', title = 'Distribution of Dislikes')

p3 <- ggplot(data = youtube, aes(like_dislike_ratio))+
  geom_histogram(bins = 30) +
  labs(x = 'Like to Dislike Ratio', title = 'Distribution of Like to Dislike Ratio')

p4 <- ggplot(data = youtube, aes(comment_count)) +
  geom_histogram(bins = 30) +
  labs(x = 'Comment Count', title = 'Distribution of Comment Count')

p1 + p2 + p3 + p4
```
```{r}
ggplot(data = youtube, aes(desc_length)) +
  geom_histogram(bins = 30) +
  labs(x = 'Length of Description', title = 'Distribution of Description Length')

ggplot(data = youtube, aes(channel_length)) +
  geom_histogram(binwidth = 1)
```


Considering that view count is positively skewed, it is no surprise that the likes, dislikes, and comment count is also positively skewed. Now we will investigate some of the predictors based on the title.

```{r}
p5 <- ggplot(data = youtube, aes(num_caps))+
  geom_histogram(bins = 30)+
  labs(x = 'Number of Capitals Letters in Title')
p6 <- ggplot(data = youtube, aes(num_exc))+
  geom_histogram(bins = 30) +
  labs(x = 'Number of ! in Title')
p7 <- ggplot(data = youtube, aes(num_qm))+
  geom_histogram(bins = 30) +
  labs(x = 'Number of ? in Title')
p8 <- ggplot(data = youtube, aes(num_period))+
  geom_histogram(bins = 30) +
  labs(x = 'Number of Periods in Title')
p9 <- ggplot(data = youtube, aes(num_dollar))+
  geom_histogram(bins = 30) +
  labs(x = 'Number of $ in Title')
p10 <- ggplot(data = youtube, aes(title_length))+
  geom_histogram(bins = 30)+
  labs(x = 'Length of Title')
p5+p6+p7+p8+p9+p10
```



```{r}
p11 <- ggplot(data = youtube, aes(video_length)) +
  geom_histogram(binwidth = 60) +
  labs(x = 'Video Length (Sec)', title = 'Distribution of Video Length')

p12 <- ggplot(data = youtube, aes(num_tags))+
  geom_histogram(binwidth = 10)+
  labs(x = 'Tags', title = 'Distribution of Number of Tags')

p13 <- ggplot(data = youtube, aes(category)) +
  geom_bar() +
  labs(x = 'Category', title = 'Distribution of Category') + 
  theme(axis.text.x = element_text(angle = 90))

p11 + p12 + p13
```
```{r}
p14 <- ggplot(data = youtube, aes(comments_disabled))+
  geom_bar()+
  labs(x = 'Comments Disabled', title = 'Distribution of Comments Disabled')

p15 <- ggplot(data = youtube, aes(ratings_disabled))+
  geom_bar()+
  labs(x = 'Ratings Disabled', title = 'Distribution of Comments Disabled')

p14 + p15
```

```{r}
p16 <- ggplot(data = youtube, aes(trending_date))+
  geom_histogram(binwidth = 14) +
  labs(title = 'Distribution of Trending Date', x = 'Trending Date')

p17<- ggplot(data = youtube, aes(publishedAt))+
  geom_histogram(binwidth = 14*60*60*24)+ # 2 week bin width
  labs(x = 'Date Published', title = 'Distribution of Date Published')

p16 + p17
```
```{r}
levels(youtube$weekday_published) = c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday','Sunday')
```



```{r}
p18 <- ggplot(data = youtube, aes(weekday_published))+
  geom_bar() +
  labs(x = 'Weekday Published', title = 'Distribution of Weekday Published')

p19 <-ggplot(data = youtube, aes(day_published)) +
  geom_histogram(binwidth = 1) +
  labs(x = 'Day Published', title = 'Distribution of Day Published')
  
p20 <- ggplot(data = youtube, aes(hour_published))+
  geom_bar(binwidth = 1) +
  labs(x = 'Hour Published', title = 'Distribution of Hour Published')

p21 <- ggplot(data = youtube, aes(trending_age))+
  geom_bar(binwidth = 1) +
  labs(x = 'TrendingAge', title = 'Distribution of Trending Age')

p18 + p19 + p20 + p21
```
Note that according to Youtube API, time and dates are in UTC time zone.

```{r}
p22 <- ggplot(data = youtube, aes(subscriberCount))+
  geom_histogram(bins = 30) +
  labs(x = 'Subscriber Count', title = 'Distribution of Subscriber Count')
p23 <- ggplot(data = youtube, aes(videoCount)) +
  geom_histogram(bins = 50) +
  labs(x = 'Number of Videos on Channel', title = 'Distribution of Video Count')

p22 + p23

```



### Bivariate

Now we will explore the relationship between our predictor variables and our response variable. As we discussed earlier, we will be using `log_views` instead of `view_count` as our response since we wish to conduct inference. We will start by observing the correlations for some of the numerical data.

```{r}
library(Hmisc)

youtube_num <- youtube %>%
  subset(select = -c(publishedAt, trending_date, comments_disabled, ratings_disabled, weekday_published, category))

cor(youtube_num)
```
Looking at the correlations, we unsurprisingly see some moderate correlation between `log_views` and `likes`, `dislikes`, and `comment_count`. This is unsurprising as naturally videos which have more viewers will have more people rating and commenting on the video. But much like with view count, the likes, dislikes, and comment count on a video will not be observed until after it has been uploading. Since we want to focus on predictor variables that are known at or before the video is uploaded, we will not use these.

Of the variables which are useful in this sense, we see the channel's subscriber count has a moderate postive correlation of 0.484 with `log_views`. We also see that the number of exclamation points has a weak negative correlation as well as the length of the description.

```{r}
b1 <- ggplot(data = youtube, aes(x = likes, y = log_views)) +
  geom_point()

b2 <- ggplot(data = youtube, aes(x = dislikes, y = log_views)) +
  geom_point()

b3 <- ggplot(data = youtube, aes(x = like_dislike_ratio, y = log_views)) +
  geom_point()

b4 <- ggplot(data = youtube, aes(x = comment_count, y = log_views)) +
  geom_point()

b1+b2+b3+b4
```

We might expect a linear relationship between likes/dislikes and the number of views. Since we took the log of views, then if we were to include likes and dislikes in our model, we might want to also take the log of likes and dislikes so we can preserved this relationship. This will likely be true for other variables as well such as subscriber count.

```{r}
b5 <- ggplot(data = youtube, aes(x = comments_disabled, y = log_views)) +
  geom_boxplot()

b6 <- ggplot(data = youtube, aes(x = ratings_disabled, y = log_views)) +
  geom_boxplot()

b5 + b6
```

It looks like disabling ratings might have an effect on `log_views` shown by the different means across videos with disabled and enabled ratings. There is also a small difference in means for videos with comments disabled.

For the sake of plotting some of these variables, we will convert the following variables to factors. 
```{r}
youtube <- youtube %>%
  mutate(num_exc = as.factor(num_exc),
         num_qm = as.factor(num_qm),
         num_period = as.factor(num_period),
         num_dollar = as.factor(num_dollar),
         hour_published = as.factor(hour_published), # convert back after eda
         trending_age = as.factor(trending_age),
         day_published = as.factor(day_published))
```




```{r}
b7 <- ggplot(data = youtube, aes(x = num_tags, y = log_views)) +
  geom_point()

b8 <- ggplot(data = youtube, aes(x = num_caps, y = log_views)) +
  geom_point()

b9 <- ggplot(data = youtube, aes(x = num_exc, y = log_views)) +
  geom_boxplot()

b10 <- ggplot(data = youtube, aes(x = num_qm, y = log_views)) +
  geom_boxplot()

b11 <- ggplot(data = youtube, aes(x = num_period, y = log_views)) +
  geom_boxplot()

b12 <- ggplot(data = youtube, aes(x = num_dollar, y = log_views)) +
  geom_boxplot()

b7+b8+b9+b10+b11+b12
```

Here it looks like the `num_qm` has little to no predictive power. But we do see there is quite a bit of variability for `log_views` across the groups for `num_dollar`, `num_period`, and `num_exc`. Also there might be a weak downward trend for `num_caps` as well. Meanwhile `num_tags` looks completely random. So the number of tags doesn't appear to help us.

```{r}
b13 <- ggplot(data = youtube, aes(x = title_length, y = log_views)) +
  geom_point(alpha = 0.4)

b14 <- ggplot(data = youtube, aes(x = desc_length, y = log_views)) +
  geom_point(alpha = 0.4)

b15 <- ggplot(data = youtube, aes(x = channel_length, y = log_views)) +
  geom_point(alpha = 0.4)

b13+b14+b15
```
Here, the scatterplot for `title_length` looks quite random, as does the scatterplot for `channel_length`. Meanwhile there looks like there might be a weak positive trend for `desc_length`.


```{r}
b15 <- ggplot(data = youtube, aes(x = weekday_published, y = log_views)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90))

b16 <- ggplot(data = youtube, aes(x = day_published, y = log_views)) +
  geom_boxplot()

b17 <- ggplot(data = youtube, aes(x = hour_published, y = log_views)) +
  geom_boxplot()

b18 <- ggplot(data = youtube, aes(x = video_length, y = log_views)) +
  geom_point(alpha = 0.2)

b19 <- ggplot(data = youtube, aes(x = category, y = log_views)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90))

b20 <- ggplot(data = youtube, aes(x = trending_age, y = log_views)) + 
  geom_boxplot()

b15+b16+b17
b18+b19+b20
```

It looks like `trending_age` and `log_views` have an association. This is expected since older videos will have had more time to get more views. That being said, the uploader will not know whether a video will trend or how long it takes to get to trending so it would not make sense to include it in our model. As for the category it looks like `log_views` varies quite a bit between different categories. Hence it might be useful to consider it in our model. For `video_length` it is hard to tell whether there is a relationship. We will construct another plot with adjusted limits

```{r}
ggplot(data = youtube, aes(x = video_length, y = log_views)) +
  geom_point(alpha = 0.3) +
  xlim(0,3000)
```
It is hard to tell whether there is a relationship between `log_views` and `video_length`. If anything, there is a very weak negative relationship between `video_length` and `log_views`, but it is hard to tell due to the difference in variability between short and long videos.



```{r}
b21 <- ggplot(data = youtube, aes(x = subscriberCount, y = log_views)) +
  geom_point()

b22 <- ggplot(data = youtube, aes(x = videoCount, y = log_views)) +
  geom_point()

b21+b22
```

As we noted earlier, it looks like there may be a postive relationship between `subscriberCount` and `log_views`. Meanwhile `videoCount` doesn't seem to be all that useful. Just to check, we will adjust the limits of our plot to get a clearer picture.

```{r}
ggplot(data = youtube, aes(x = videoCount, y = log_views)) +
  geom_point(alpha = 0.3)+
  xlim(0,10000)
```

Based off of this second plot, `videoCount` still does not appear to be all that useful. It doesn't look like the number of video's a channel has affects the number of views a trending video might have. 

```{r}
b23 <- ggplot(data = youtube, aes(x = weekday_published, y =log_views)) +
  geom_boxplot()

b24 <- ggplot(data = youtube, aes(x = hour_published, y =log_views)) +
  geom_boxplot()

b25 <- ggplot(data = youtube, aes(x = day_published, y =log_views)) +
  geom_boxplot()

b24+b23/b25

youtube %>% group_by(weekday_published) %>%
  summarise(across(log_views, mean))
```

Here it looks like the day of the week might be useful in predicting `log_views`. While the boxplot doesn't look all that helpful, looking at the means across the weekdays, `log_views` for Friday and Saturday are closer to around 14.2 while that mean is closer to 13.8 for Mondays and Tuesdays. While this seems like a very small difference, this is in fact a $e^{14.2}-e^{13.8} = 484255$ view difference which is quite substantial. Looking at the box plots, there is quite a bit of variability in `log_views` across `hour_published`. So `hour_published` might be useful in our model. The same could be said about `day_published` as well.

### Multivariate

Now that we have identified some potentially useful predictor variables that may be useful, we will explore some of the relationships between them. As we noted in the previous section of our EDA, `subscriberCount`, `category`, `weekday_published`, `hour_published` and a few other variables. We will explore the relationships between some of these variables here.

We will start with subscriber count. It is possible that more popular channels tend to have videos of a certain length or tend to upload at specific times or days. It is also possible that subscriber counts vary across video categories.

```{r}
m1 <- ggplot(data = youtube, aes(x = category, y = subscriberCount)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90))

m2 <- ggplot(data = youtube, aes(x = weekday_published, y = subscriberCount)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90))

m3 <- ggplot(data = youtube, aes(x = hour_published, y = subscriberCount)) +
  geom_boxplot()

m4 <- ggplot(data = youtube, aes(x = video_length, y = subscriberCount)) +
  geom_point(alpha = 0.2)

m1+m2
m3+m4
```

Looking at the above plots, it looks like the `hour_published` and `weekday_published` are potential covariates. Meanwhile `video_length` doesn't appear to be associated with `subscriberCount`. This isn't super surprising as the in the correlation matrix, we found the correlation between `subscriberCount` and `video_length` to be quite small.

Now we will look at `video_length`. A few possible relationships may be with respect to `desc_length` and `title_length`. Potentially longer videos might have more characters in the description or title since there might be more content to summarize in these videos.

```{r}
m5 <- ggplot(data = youtube, aes(x = desc_length, y = video_length)) +
  geom_point() + xlim(0,2000) + ylim(0, 3000)# note: limits adjusted to zoom in

m6 <- ggplot(data = youtube, aes(x = title_length, y = video_length)) +
  geom_point() + ylim(0,2000)

m5+m6
```
There doesn't seem to be much of a relationship in these cases. If anything there might be a very weak positive relationship between `title_length` and `video_length`.

One thing we would expect is that there is a relationship between many of the variables related to title. We would expect variables like `title_length`, `num_exc`, `num_qm` and so on to be related.

```{r}
m7 <- ggplot(data = youtube, aes(fill = num_exc, y = num_qm))+
  geom_bar(position = 'fill')

m8 <- ggplot(data = youtube, aes(x = title_length, y = num_exc))+
  geom_boxplot()

m9 <- ggplot(data = youtube, aes(x = title_length, y = num_qm))+
  geom_boxplot()

m10 <- ggplot(data = youtube, aes(x = title_length, y = num_period))+
  geom_boxplot()

m11 <- ggplot(data = youtube, aes(x = title_length, y = num_caps))+
  geom_point()

m7 +m8
m8+m9+m10+m11
```
As we can see, many of these variables having to do with the title are associated. `title_length` and `num_caps` seem most obvious with a fairly clear positive linear association.

Once we decide on what variables to use in our model, we can further investigate whether any other predictors are associated with one another.

## Modeling

As discussed earlier, the distribution for `view_count` is heavily skewed. Since the purpose of our analysis is to conduct inference on our model, we would like our response variable to be approximately normally distributed. Hence we will be using `log_views` instead of `view_count` for our predictions. As for the predictor variables, we only want to include predictor variables that the uploader might have knowledge of before uploading. So we will not be using variables such as `likes`, `dislikes`, and `trending_age` since the uploader will not know the values of these variables until the video is uploaded. This narrows down the variables we can use considerably. Following from the EDA we will build a model using the following predictor variables:

`subscriberCount`, `category`, `comments_disabled`, `ratings_disabled`, `num_caps`, `num_exc`, `num_period`, `num_dollar`, `desc_length`, `hour_published`, `weekday_published`, `video_length`

We can quickly build a model to predict `log_views` with these predictor variables with no interaction terms. But as we noted in the Exploratory data analysis, there seems to be a weak linear relationship between `num_exc`, `num_period`, `num_dollar` and `log_views`. So we will consider 2 separate models: one with those variables as factors, the other with those variables as integers. For the model with those variables treated as integers, we will use the data `youtube_int`.

```{r}
youtube_int <- youtube %>%
  mutate(num_exc = as.integer(num_exc),
         num_period = as.integer(num_period),
         num_dollar = as.integer(num_dollar))
```

Now we will create 2 models both using all of the variables we mentioned above. In `full_model_factor`, we will treat `num_exc`, `num_period`, and `num_dollar` as factors. In `full_model_int` they will be treated as integers.

```{r}
full_model_factor <- lm(log_views ~ subscriberCount + category + comments_disabled + ratings_disabled + num_caps + num_exc + num_period + num_dollar + desc_length + hour_published + weekday_published + day_published + video_length, data = youtube)

full_model_int <- lm(log_views ~ subscriberCount + category + comments_disabled + ratings_disabled + num_caps + num_exc + num_period + num_dollar + desc_length + hour_published + weekday_published + day_published + video_length, data = youtube_int)

glance(full_model_factor)
glance(full_model_int)
```

Looking at the AIC and BIC of our 2 models, it looks like our model with `full_model_int` is considered better. This isn't super surprising as they have fairly close values of $R^2$ but `full_model_factor` has 92 degrees of freedom compared to `full_model_int` with only 81. Seeing as AIC and BIC penalizes more complex models, this is expected. Since the difference in $R^2$ is still fairly small, we will be building off of `full_model_int` Now we will try running backwards selection on our model using AIC.

```{r}
model_back_selection <- step(full_model_int, direction = 'backward')
glance(model_back_selection)
```
Thus, according to the backwards selection our best model has the predictors `subscriberCount`, `num_exc`, `video_length`, `desc_length`, `num_caps`, `category`. But while we have a small AIC, our $R^2$ has dropped significantly from its level of about $0.32$. So we are going to consider adding back some of our variables which had quite a bit of predictive power.

```{r}
aic_model_weekday <- lm(log_views ~ subscriberCount + category + num_caps + num_exc + desc_length + video_length+ weekday_published, data = youtube_int)
aic_model_hr <- lm(log_views ~ subscriberCount + category + num_caps + num_exc + desc_length + video_length + hour_published, data = youtube_int)
aic_model_day <- lm(log_views ~ subscriberCount + category + num_caps + num_exc + desc_length + video_length + day_published, data = youtube_int)
aic_model_weekday_hr <- lm(log_views ~ subscriberCount + category + num_caps + num_exc + desc_length + video_length + weekday_published + hour_published, data = youtube_int)


glance(aic_model_weekday)
glance(aic_model_hr)
glance(aic_model_day)
glance(aic_model_weekday_hr)

```
As shown above, it looks like adding back `weekday_published` and `hour_published` has increased our $R^2$ close to 0.3 shown in our model `aic_model_weekday_hr`. While our AIC has increased, $R^2$ has increased a fairly significant amount. Now we will consider adding in some interaction terms. So in our model we have `subscriberCount`,`category`,`num_caps`,`num_exc`,`desc_length`,`video_length`,`weekday_published`, and `hour_published`. As we noted in the EDA, the variables relating to the title seem to be associated. So we might want to include the interaction between `num_caps` and `num_exc` in our model. Also from the EDA, we might want to include the interaction between `subscriberCount` and `hour_published`. Additionally, we will include the interaction between `category` and `subscriberCount`.

```{r}
model <- lm(log_views ~ subscriberCount * hour_published + category + num_caps * num_exc + desc_length + video_length + weekday_published + subscriberCount*category,
            data = youtube_int)
tidy(model, conf.int = TRUE) %>%
  kable(digits = 3)
```
Looking at the model output, we see that the interactions between `subscriberCount` and `hour_published` mostly seem to have fairly large p-values. While there are a few with small p-values, such as `subscriberCount:hour_published1` with a p-value of 0.008, looking at the confidence interval, it is extremely close to 0. So while the p-value suggests we include the interaction in our model, since the confidence interval is so close to 0, we will not include it. Also, the interaction between `num_caps` and `num_exc` has a large p-value of 0.404 and the confidence interval captures 0. So that interaction will not be included in the final model as well. We reach the same conclusions for the interactions between `subscriberCount` and `category`. So our model will not include any interaction terms. But looking at the coefficient for `subscriberCount`, it is essentially 0. Looking at the confidence interval, it is also extremely close to 0. So while the p-value suggests that `subscriberCount` is important, we will remove it from our model.

```{r}
model <- lm(log_views ~ category + num_caps + num_exc + desc_length + video_length + weekday_published + hour_published, data = youtube_int)
```


But one thing we have not addressed is outliers. Just looking at the EDA, we have quite a few outliers in both the response variable and many of the predictors. Since this is the case it is quite likely there are a few leverage points. 

```{r}
model_aug <- augment(model) %>%
  mutate(obs_num = 1:n())
```

Now we will investigate the leverage for each of the observations in our dataset. We will set the threshold for leverage at
\[\frac{2(p+1)}{n} = \frac{2(8+1)}{974} = 0.018\]
We can plot the leverage for each observation.
```{r}
ggplot(data = model_aug, aes(x = obs_num, y = .hat))+
  geom_point() +
  geom_hline(yintercept = 0.018, color = 'red') +
  labs(x = 'Observation Number', y = 'Leverage')
```
Weirdly it looks like all observations lie above our threshold for 

Now we will look at the standardized residuals as well as the cooks distance.

```{r}
ggplot(data = model_aug, aes(x = .fitted, y = .std.resid))+
  geom_point() +
  geom_hline(yintercept = -2, color = 'red')+
  geom_hline(yintercept = 2, color = 'red') +
  labs(x = 'Fitted Values', y='Standard Residuals')

ggplot(data = model_aug, aes(x = obs_num, y = .cooksd))+
  geom_point() +
  geom_hline(yintercept = 1, color = 'red')+
  labs(x = 'Observation Number', y='Cooks Distance')
```

```{r}
model_aug[which(abs(model_aug$.cooksd)>1),]

model_aug[which(abs(model_aug$.std.resid)>2),]

```

It looks like we have 48 observations which lie outside our standard residual threshold. Meanwhile we have a no observations lying above the threshold for Cooks Distance. Since this is the case and these observations are perfectly valid videos, we will not remove them from the dataset.

Hence we can get our final model

```{r}
final_model <- lm(log_views ~  category + num_caps + num_exc + desc_length + video_length + weekday_published + hour_published, data = youtube_int)

tidy(final_model, conf.int = TRUE) %>% 
  kable(digits = 3)
```
Since we took the log of view count, all of our interpretations for the coefficients need to be adjusted. We will not interprate all of the coefficients, but we will interpret a few. For `num_exc`, we have a coefficient of -0.224. Interpreting this coefficient, this means that holding all else constant, if we were to increase the number of exclamation points in the title by 1, we would expect the number of views to be multiplied by a factor of $e^{-0.224} = 0.799$. Interpreting `weekday_publishedSaturday` with a coefficient of 0.378, this means that holding all else constant, if we were to change the publish date of the same video to Saturday from Monday, we would expect the number of views to be multiplied by a factor of $e^{0.378} = 1.459$.

The coefficients for the different categories of `weekday_published` are quite interesting. It seems that holding all else constant, changing the day published from Monday to any other day except Tuesday is associated with an increase in viewership. Looking at `hour_published4`, it looks like 4 and 7 am UTC lead to the largest increases in viewership from midnight UTC holding all else constant. These times correspond to 8 and 11 pm PST. Since we are looking at videos in the US, this might suggest that youtube users are most active during those times.

## Checking Assumptions

Using our full model, we will check the assumptions for our model. We will start by looking at linearity

